{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42730c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import edge_tts\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Wrap your entire script in SSML <speak> … </speak>,\n",
    "# inserting <break time=\"Xs\"/> wherever you need a pause.\n",
    "ssml_text = \"\"\"\n",
    "\n",
    "  Good Afternoon Everyone.\n",
    "  Today I'll walk you through how we can leverage Python to acquire, clean, analyze, and visualize financial time-series data - specifically, S and P 500 ETF Spy prices.\n",
    "\n",
    "  Let's dive in.\n",
    "\n",
    "  Agenda\n",
    "  Here's our roadmap for the next 15 minutes. I'll go over:\n",
    "  Why I chose Spy data.\n",
    "  How to set up the environment in VS Code.\n",
    "  Pulling in and inspecting the data.\n",
    "  Cleaning and transforming it.\n",
    "  Performing analysis and creating visuals.\n",
    "  And lastly exporting results and then we'll wrap things up.\n",
    "  So feel free to jot down questions; we'll leave time at the end.\n",
    "\n",
    "  Why Spy?\n",
    "  Why did I use this dataset? First, it's a household name in finance - everyone gets stock charts.\n",
    "  Second, it provides multiple fields - Open, High, Low, Close, and Volume - so we can demonstrate several transformations in one dataset.\n",
    "  Finally, it mirrors real-world workflows: analysts ingest live market data, clean it, calculate indicators, and share insights rapidly.\n",
    "\n",
    "  Environment Setup\n",
    "  Think of VS Code like a digital workshop that runs on Windows, or Mac—all your coworkers can use it. It comes with built-in tools to manage your project history and a ‘step-through’ feature that lets you pause your program line by line to look under the hood.\n",
    "  Now imagine you have a special toolbox for this project only. A virtual environment is that toolbox—it keeps all the parts you need (and only those parts) in one spot.\n",
    "  Then install our core libraries: First I used Pandas: This is your spreadsheet brain—makes it easy to load, slice, and dice tables of numbers. Next I used yfinance: Think of it as a faucet you turn on to pull in stock prices directly from Yahoo Finance—no manual downloads. Lastly I used Matplotlib: This is our chart-drawing tool—quickly turns columns of numbers into lines and bars so you can actually see what’s happening.\n",
    "\n",
    "  Data Ingestion\n",
    "  Now there are multiple ways to acquire data. For this project I used Option A: Automatic pull with yfinance. You don’t have to go download a file by hand each time; your script just reaches out and grabs the latest numbers.\n",
    "  You can also use Option B: Downloaded CSV file. Sometimes company rules block your script from talking to the internet. In that case, you manually grab a spreadsheet (CSV) from Yahoo Finance ahead of time—like picking up a paper report—and your code reads that instead.\n",
    "  No matter how the data arrives, pandas.DataFrame is like a supercharged spreadsheet inside Python. It keeps rows and columns neatly organized in memory, so you can sort, filter, and calculate on the fly just like you would in Excel, but with code.\n",
    "\n",
    "  Data Inspection\n",
    "  Imagine opening the front cover of a big book and reading the first few pages to make sure it’s the right book and the print looks clear. df head shows you the first handful of rows so you can confirm you have the right columns (like “Date,” “Close,” “Volume”) and that the data looks sensible. Next think of df info like flipping to the table of contents and index to see how many chapters there are, if any pages are missing, and what types of sections you have (text vs. images). df info tells you how many entries each column has (so you catch any blanks), and what kind of data is in them (numbers vs. text). That way you spot any unexpected gaps or formatting issues right away. Lastly, df describe is like glancing at a summary on the back cover: ‘Here’s the average, the highest, the lowest, and the middle values.’ df describe gives you basic stats—average price, the minimum and maximum, and where most values fall—so you can instantly spot oddball numbers (like zero volume days or a price that’s way out of range).\n",
    "\n",
    "  Cleaning and Transformation\n",
    "  First, we put our data in time order—like arranging photos by the day they were taken. If you don’t, any calculations that look at “the last 20 days” will get confused, because the days won’t line up correctly. Sometimes the market is closed for a holiday or there’s a missing entry—and that leaves a blank spot in our table. Forward-fill just says, ‘If today’s data is missing, copy yesterday’s data instead.’ It’s like assuming the price stayed the same on days when nobody traded. Next I calculated the key numbers: First, Daily Return: This is how much the price changed from one day to the next, as a percentage. Think of it like checking your step count change day-over-day: it shows how jumpy or calm the market was. Second, 20-day SMA: This is like taking the average of your last 20 days of step counts to see your overall fitness trend—up, down, or flat—so you don’t obsess over each single noisy data point. Lastly, Cumulative Return: Imagine you put $1 in the market on day one. This number tells you how much that dollar would have grown (or shrunk) over time. It gives you the big-picture payoff instead of just daily blips.\n",
    "\n",
    "  Visualization\n",
    "  In my script I created two basic charts. First, a Line chart: this is Plot Close vs. 20-day SMA—you’ll see where price crosses its trend line, signaling potential buy/sell points. Second, a Histogram: this is the Daily Return distribution—gauges risk by showing how often big up/down days occur. These visuals can be exported directly or embedded in reports, dashboards, or notebooks. Once you’ve got these pictures, you can save them as image files or put them straight into your slide decks, reports, or online dashboards. No extra work—just copy-and-paste.\n",
    "\n",
    "  Export and Integration\n",
    "  With one line df to csv or df to excel —you archive your cleaned data or share with clients. We can optionally wrap our pipeline in a tiny web service using Flask—think of it as a ‘data help desk.’ Instead of emailing Excel files, any tool in your ecosystem just calls a URL and instantly gets back the freshest numbers or charts. It’s fast, automated, and removes the manual hand-offs.\n",
    "\n",
    "  Conclusion\n",
    "  This same pipeline pattern scales to any client dataset—logs, inventory, or financial management—you swap in the data source and everything else just works. In my current role I design and deploy data workflows that grab raw inputs, clean them up, and enrich them automatically—architecting end-to-end data flows. We put checks and controls in place so every number we report has an audit trail, and our leadership can rely on it without second-guessing—ensuring data quality and governance. By building ready-made dashboards and exportable datasets, we can empower teams to run their own analyses—speeding up decisions across the board. Once we’ve built the core pipeline, you can plug in new data sources or AI/ML modules without redoing the plumbing—enabling reuse and extensibility.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "async def make_tts():\n",
    "    # Use any neural voice you like\n",
    "    communicate = edge_tts.Communicate(ssml_text, voice=\"en-US-RogerNeural\")\n",
    "    await communicate.save(\"/your_directory/presentation_brief.mp3\")\n",
    "\n",
    "# In Jupyter/IPython you can await directly\n",
    "await make_tts()\n",
    "\n",
    "# Speed it up slightly\n",
    "AudioSegment.converter = \"/opt/homebrew/bin/ffmpeg\"\n",
    "AudioSegment.ffprobe   = \"/opt/homebrew/bin/ffprobe\"\n",
    "\n",
    "sound  = AudioSegment.from_file(\"/your_directory/spresentation_brief.mp3\")\n",
    "faster = sound.speedup(playback_speed=1.2)\n",
    "faster.export(\"your_directory/spresentation_brief_faster.mp3\", format=\"mp3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6899bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import edge_tts\n",
    "\n",
    "async def list_voices():\n",
    "    voices = await edge_tts.list_voices()\n",
    "    # print just the voice ID and its locale/friendly name\n",
    "    for v in voices:\n",
    "        print(f\"{v['ShortName']:<25} {v['Locale']}  —  {v.get('FriendlyName','')}\")\n",
    "        \n",
    "# in a notebook supporting top-level await, just:\n",
    "await list_voices()\n",
    "# otherwise use nest_asyncio or get_event_loop().run_until_complete(...)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
